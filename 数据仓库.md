DWH(Data WareHouse) 数据仓库



事实表：数据聚合后依据某个维度生成的结果表

维度表：要对数据进行分析时所用的一个量, 比如你要分析产品销售情况, 你可以选择按类别来进行分析,或按区域来分析

可量化数据或数字数据是事实表的最佳选择



**ETL**，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。**ETL**一词较常用在[数据仓库](https://baike.baidu.com/item/数据仓库)，但其对象并不限于[数据仓库](https://baike.baidu.com/item/数据仓库)。

kimball框架

Inmon

Corporate Information Factory (CIF) 建立在3NF规范化数据库上，然后允许Data Marts对文档化的数据进行规范化。

混合Kimball总线和Inmon CIF模型对于企业数据仓库仍然适用，其数据保存在3NF中，即使规范化数据表可能不是BI报表的最佳选择。

OLTP(联机事务处理)和*OLAP*(联机分析处理)

summarize，split，roll-up，slice，drill-down，dice，cut

slice（切片）：将其中一个维度固定为一个值

dice（切块）：限制值的范围来计算多维数据集

roll-up（上滚）：汇总或合并值，并减少行数或列数。

drill-down(下钻)：分解值并增加行数或列数。

**ROLAP**表示基于关系数据库的OLAP实现（Relational OLAP）

**MOLAP**表示基于多维数据组织的OLAP实现（Multidimensional OLAP）

**HOLAP**表示基于混合数据组织的OLAP实现（Hybrid OLAP）



用户角色管理：https://console.aws.amazon.com/iam/home#/home

redshift：https://us-west-2.console.aws.amazon.com/redshift/home?region=us-west-2#

存储：https://console.aws.amazon.com/s3/home?region=us-west-2

数据库：https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2#

IAM服务：Identity and Access Management 身份识别与访问管理

虚拟私有云（*VPC*）



Redshift：列导向，自我管理，并行

计算节点：dc1,dc2

存储节点：ds2

boto3:python aws sdk



发行方式：

EVEN distribution:一个表在切片上进行分区，这样每个切片将具有几乎相等数量的记录。

ALL distribution:ALL分发样式将在所有片上复制一个表

AUTO distribution

KEY distribution:将具有相似值的行放置在同一切片中

```python
import os 
from time import time
import configparser
import matplotlib.pyplot as plt
import pandas as pd
import boto3
import json
from botocore.exceptions import ClientError
config = configparser.ConfigParser()
config.read_file(open('dwh.cfg'))
KEY=config.get('AWS','key')
SECRET= config.get('AWS','secret')
DWH_CLUSTER_TYPE       = config.get("DWH","DWH_CLUSTER_TYPE")
DWH_NUM_NODES          = config.get("DWH","DWH_NUM_NODES")
DWH_NODE_TYPE          = config.get("DWH","DWH_NODE_TYPE")
DWH_CLUSTER_IDENTIFIER = config.get("DWH","DWH_CLUSTER_IDENTIFIER")
DWH_DB= config.get("DWH","DWH_DB")
DWH_DB_USER= config.get("DWH","DWH_DB_USER")
DWH_DB_PASSWORD= config.get("DWH","DWH_DB_PASSWORD")
DWH_PORT = config.get("DWH","DWH_PORT")
DWH_IAM_ROLE_NAME      = config.get("DWH", "DWH_IAM_ROLE_NAME")
pd.DataFrame({"Param":
                  ["DWH_CLUSTER_TYPE", "DWH_NUM_NODES", "DWH_NODE_TYPE", "DWH_CLUSTER_IDENTIFIER", "DWH_DB", "DWH_DB_USER", "DWH_DB_PASSWORD", "DWH_PORT", "DWH_IAM_ROLE_NAME"],
              "Value":
                  [DWH_CLUSTER_TYPE, DWH_NUM_NODES, DWH_NODE_TYPE, DWH_CLUSTER_IDENTIFIER, DWH_DB, DWH_DB_USER, DWH_DB_PASSWORD, DWH_PORT, DWH_IAM_ROLE_NAME]
             })
ec2 = boto3.resource('ec2',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                    )
s3 = boto3.resource('s3',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                   )
iam = boto3.client('iam',aws_access_key_id=KEY,
                     aws_secret_access_key=SECRET,
                     region_name='us-west-2'
                  )
redshift = boto3.client('redshift',
                       region_name="us-west-2",
                       aws_access_key_id=KEY,
                       aws_secret_access_key=SECRET
                       )
# check out the sample data sources on S3
sampleDbBucket =  s3.Bucket("awssampledbuswest2")
for obj in sampleDbBucket.objects.filter(Prefix="ssbgz"):
    print(obj)
# Create an IAM Role that makes Redshift able to access S3 bucket (ReadOnly)
#1.1 Create the role, 
try:
    print("1.1 Creating a new IAM Role") 
    dwhRole = iam.create_role(
        Path='/',
        RoleName=DWH_IAM_ROLE_NAME,
        Description = "Allows Redshift clusters to call AWS services on your behalf.",
        AssumeRolePolicyDocument=json.dumps(
            {'Statement': [{'Action': 'sts:AssumeRole',
               'Effect': 'Allow',
               'Principal': {'Service': 'redshift.amazonaws.com'}}],
             'Version': '2012-10-17'})
    )    
except Exception as e:
    print(e)  
print("1.2 Attaching Policy")
iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,
                       PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
                      )['ResponseMetadata']['HTTPStatusCode']
print("1.3 Get the IAM role ARN")
roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']
print(roleArn)

#Create a RedShift Cluster
try:
    response = redshift.create_cluster(        
        #HW
        ClusterType=DWH_CLUSTER_TYPE,
        NodeType=DWH_NODE_TYPE,
        NumberOfNodes=int(DWH_NUM_NODES),
        #Identifiers & Credentials
        DBName=DWH_DB,
        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,
        MasterUsername=DWH_DB_USER,
        MasterUserPassword=DWH_DB_PASSWORD,
        #Roles (for s3 access)
        IamRoles=[roleArn]  
    )
except Exception as e:
    print(e)

# Describe the cluster to see its status
def prettyRedshiftProps(props):
    pd.set_option('display.max_colwidth', -1)
    keysToShow = ["ClusterIdentifier", "NodeType", "ClusterStatus", "MasterUsername", "DBName", "Endpoint", "NumberOfNodes", 'VpcId']
    x = [(k, v) for k,v in props.items() if k in keysToShow]
    return pd.DataFrame(data=x, columns=["Key", "Value"])
myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]
prettyRedshiftProps(myClusterProps)
# dwhcluster.csmamz5zxmle.us-west-2.redshift.amazonaws.com
DWH_ENDPOINT = myClusterProps['Endpoint']['Address']
# arn:aws:iam::988332130976:role/dwhRole
DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']

# Open an incoming TCP port to access the cluster ednpoint
try:
    vpc = ec2.Vpc(id=myClusterProps['VpcId'])
    defaultSg = list(vpc.security_groups.all())[0]
    print(defaultSg)
    defaultSg.authorize_ingress(
        GroupName=defaultSg.group_name,
        CidrIp='0.0.0.0/0',
        IpProtocol='TCP',
        FromPort=int(DWH_PORT),
        ToPort=int(DWH_PORT)
    )
except Exception as e:
    print(e)


# connect to cluster
conn_string="postgresql://{}:{}@{}:{}/{}".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)
print(conn_string)
%sql $conn_string
def loadTables(schema, tables):
    loadTimes = []
    SQL_SET_SCEMA = "SET search_path TO {};".format(schema)
    %sql $SQL_SET_SCEMA
    
    for table in tables:
        SQL_COPY = """
			copy {} from 's3://awssampledbuswest2/ssbgz/{}' 
			credentials 'aws_iam_role={}'
			gzip region 'us-west-2';
        """.format(table,table, DWH_ROLE_ARN)

        print("======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======".format(table, schema))
        print(SQL_COPY)

        t0 = time()
        %sql $SQL_COPY
        loadTime = time()-t0
        loadTimes.append(loadTime)

        print("=== DONE IN: {0:.2f} sec\n".format(loadTime))
    return pd.DataFrame({"table":tables, "loadtime_"+schema:loadTimes}).set_index('table')

# delete cluster
redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)
# delete role
iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn="arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess")
iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)
```

```txt
[AWS]
KEY=
SECRET=

[DWH] 
DWH_CLUSTER_TYPE=multi-node
DWH_NUM_NODES=4
DWH_NODE_TYPE=dc2.large

DWH_IAM_ROLE_NAME=dwhRole
DWH_CLUSTER_IDENTIFIER=dwhCluster
DWH_DB=dwh
DWH_DB_USER=dwhuser
DWH_DB_PASSWORD=Passw0rd
DWH_PORT=5439
```

### 创建用户

https://console.aws.amazon.com/iam/home#/home

创建dwhadmin用户，勾选编程权限，赋予AdministratorAccess权限，复制公钥和私钥







# FAQ

1. 连接不上数据库

```python
conn = psycopg2.connect("host={} dbname={} user={} password={} port={}".format(*config['CLUSTER'].values()))
```

```xml
[CLUSTER]
HOST= redshift-cluster-alan.chsxgdnjzwry.us-west-2.redshift.amazonaws.com
DB_NAME= dwh
DB_USER= ***
DB_PASSWORD= ***
DB_PORT= 5439
```

原因：1. 创建redshift集群时，在'网络和安全'->'VPC安全组'中，需要创建自定义安全组，并配置[入站规则](https://classroom.udacity.com/nanodegrees/nd027/parts/69a25b76-3ebd-4b72-b7cb-03d82da12844/modules/445568fc-578d-4d3e-ab9c-2d186728ab22/lessons/53e6c5d3-c9bb-4938-9133-bf8c6bfad3da/concepts/2609fcec-122e-4780-bfff-510713320800)；2.redshift的VPC设置下，“可公开访问的”，默认值为“否”，将其更改为“是”

2. psycopg2.InternalError: Load into table 'staging_events' failed.Check 'stl_load_errors' system table for details

redshift->Query editor->select * from stl_load_errors;

3. psycopg2.NotSupportedError: ROW expression, implicit or explicit, is not supported in target list

The error is telling that your syntax is wrong. Please try to remove those unnecessary parentheses.

Like, from this one

```
 1INSERT INTO songs(song_id, title, artist_id, year, duration)  2    SELECT(song_id,title,artist_id,year,duration) FROM staging_songs;
```

to this one

```
 1INSERT INTO songs(song_id, title, artist_id, year, duration)  2SELECT song_id,title,artist_id,year,duration FROM staging_songs;
```