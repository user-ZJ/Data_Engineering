DWH(Data WareHouse) 数据仓库



事实表：数据聚合后依据某个维度生成的结果表

维度表：要对数据进行分析时所用的一个量, 比如你要分析产品销售情况, 你可以选择按类别来进行分析,或按区域来分析

可量化数据或数字数据是事实表的最佳选择



**ETL**，是英文Extract-Transform-Load的缩写，用来描述将数据从来源端经过抽取（extract）、转换（transform）、加载（load）至目的端的过程。**ETL**一词较常用在[数据仓库](https://baike.baidu.com/item/数据仓库)，但其对象并不限于[数据仓库](https://baike.baidu.com/item/数据仓库)。

kimball框架

Inmon

Corporate Information Factory (CIF) 建立在3NF规范化数据库上，然后允许Data Marts对文档化的数据进行规范化。

混合Kimball总线和Inmon CIF模型对于企业数据仓库仍然适用，其数据保存在3NF中，即使规范化数据表可能不是BI报表的最佳选择。

OLTP(联机事务处理)和*OLAP*(联机分析处理)

summarize，split，roll-up，slice，drill-down，dice，cut

slice（切片）：将其中一个维度固定为一个值

dice（切块）：限制值的范围来计算多维数据集

roll-up（上滚）：汇总或合并值，并减少行数或列数。

drill-down(下钻)：分解值并增加行数或列数。

**ROLAP**表示基于关系数据库的OLAP实现（Relational OLAP）

**MOLAP**表示基于多维数据组织的OLAP实现（Multidimensional OLAP）

**HOLAP**表示基于混合数据组织的OLAP实现（Hybrid OLAP）



用户角色管理：https://console.aws.amazon.com/iam/home#/home

redshift：https://us-west-2.console.aws.amazon.com/redshift/home?region=us-west-2#

存储：https://console.aws.amazon.com/s3/home?region=us-west-2

数据库：https://us-west-2.console.aws.amazon.com/rds/home?region=us-west-2#

IAM服务：Identity and Access Management 身份识别与访问管理

虚拟私有云（*VPC*）



Redshift：列导向，自我管理，并行

计算节点：dc1,dc2

存储节点：ds2

boto3:python aws sdk



发行方式：

EVEN distribution:一个表在切片上进行分区，这样每个切片将具有几乎相等数量的记录。

ALL distribution:ALL分发样式将在所有片上复制一个表

AUTO distribution

KEY distribution:将具有相似值的行放置在同一切片中

```python
import os 
from time import time
import configparser
import matplotlib.pyplot as plt
import pandas as pd
config = configparser.ConfigParser()
config.read_file(open('dwh.cfg'))
KEY=config.get('AWS','key')
SECRET= config.get('AWS','secret')
DWH_DB= config.get("DWH","DWH_DB")
DWH_DB_USER= config.get("DWH","DWH_DB_USER")
DWH_DB_PASSWORD= config.get("DWH","DWH_DB_PASSWORD")
DWH_PORT = config.get("DWH","DWH_PORT")
# FILL IN THE REDSHIFT ENDPOINT HERE
DWH_ENDPOINT="dwhcluster.csmamz5zxmle.us-west-2.redshift.amazonaws.com"
#FILL IN THE IAM ROLE ARN you got in step 2.2 of the previous exercise
DWH_ROLE_ARN="arn:aws:iam::988332130976:role/dwhRole"
conn_string="postgresql://{}:{}@{}:{}/{}".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)
print(conn_string)
%sql $conn_string
def loadTables(schema, tables):
    loadTimes = []
    SQL_SET_SCEMA = "SET search_path TO {};".format(schema)
    %sql $SQL_SET_SCEMA
    
    for table in tables:
        SQL_COPY = """
			copy {} from 's3://awssampledbuswest2/ssbgz/{}' 
			credentials 'aws_iam_role={}'
			gzip region 'us-west-2';
        """.format(table,table, DWH_ROLE_ARN)

        print("======= LOADING TABLE: ** {} ** IN SCHEMA ==> {} =======".format(table, schema))
        print(SQL_COPY)

        t0 = time()
        %sql $SQL_COPY
        loadTime = time()-t0
        loadTimes.append(loadTime)

        print("=== DONE IN: {0:.2f} sec\n".format(loadTime))
    return pd.DataFrame({"table":tables, "loadtime_"+schema:loadTimes}).set_index('table')
```

