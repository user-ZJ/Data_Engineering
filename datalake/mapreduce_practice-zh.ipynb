{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce\n",
    "\n",
    "MapReduce 是通过机器集群来分析大量数据的编程技术。在这个 Jupyter notebook 中，你会对 Hadoop MapReduce 的工作原理有一些认识。但是，这个 notebook 只在本地运行而不是在机器集群上。\n",
    "\n",
    "Hadoop 和 Spark 最大的不同是 Spark 会尽可能通过内存计算，这样就避免了在集群间来回传输数据了。Hadoop 会把中间计算结果写到硬盘里，这样效率会低一些。Hadoop 比起 Spark 算更老的技术，但也是大数据技术的一个里程碑。\n",
    "\n",
    "如果你点击工作空间顶部的 Jupyter notebook 图标，你就会跳转到工作空间的文件夹。这是个文本文件，其中每行都代表了在 Sparkify 这个 app 里播放的歌曲。你会看到一个叫做 \"songplays.txt\" 的文件。MapReduce 代码会统计每首歌播放的次数。换句话说，这个代码会统计每首歌的歌名出现在列表中的次数。\n",
    "\n",
    "\n",
    "# MapReduce VS Hadoop MapReduce\n",
    "\n",
    "这两个术语很相似，但不是指同一个东西哦！MapReduce 是个编程技术，Hadoop MapReduce 是这个编程技术的一个具体的实现方式。\n",
    "\n",
    "有些语法看上去有点奇怪，所以一定要把每段的解释和注释读一遍。在后面的课程里你会学习更多的语法。\n",
    "\n",
    "把下面的代码都跑一遍，看看输出结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mrjob library. This package is for running MapReduce jobs with Python\n",
    "# In Jupyter notebooks, \"!\" runs terminal commands from inside notebooks \n",
    "\n",
    "! pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    \n",
    "    # the map step: each line in the txt file is read as a key, value pair\n",
    "    # in this case, each line in the txt file only contains a value but no key\n",
    "    # _ means that in this case, there is no key for each line\n",
    "    def mapper(self, _, song):\n",
    "        # output each line as a tuple of (song_names, 1) \n",
    "        yield (song, 1)\n",
    "\n",
    "    # the reduce step: combine all tuples with the same key\n",
    "    # in this case, the key is the song name\n",
    "    # then sum all the values of the tuple, which will give the total song plays\n",
    "    def reducer(self, key, values):\n",
    "        yield (key, sum(values))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the code as a terminal command\n",
    "! python wordcount.py songplays.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码结果的总结\n",
    "\n",
    "songplays.txt 有如下一列的歌曲：\n",
    "\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Broken Networks\n",
    "Data House Rock\n",
    "etc.....\n",
    "\n",
    "在 map 阶段，代码一次会读取文本文件的一行，然后会产出一堆如下的元组：\n",
    "\n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Broken Networks, 1)  \n",
    "(Data House Rock, 1)  \n",
    "etc.....\n",
    "\n",
    "Finally, the reduce step combines all of the values by keys and sums the values:  \n",
    "\n",
    "(Deep Dreams, \\[1, 1, 1, 1, 1, 1, ... \\])  \n",
    "(Data House Rock, \\[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\\])  \n",
    "(Broken Networks, \\[1, 1, 1, ...\\]  \n",
    "\n",
    "With the output \n",
    "\n",
    "(Deep Dreams, 1131)  \n",
    "(Data House Rock, 510)  \n",
    "(Broken Networks, 828)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
